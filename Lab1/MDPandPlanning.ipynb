{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid is defined with lengh and width. Total number of states and actions are calculated. \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "L = 8 # grid length constant\n",
    "W = 8 # grid width constant\n",
    "\n",
    "S = [] # initialize the state space\n",
    "\n",
    "# iterate all possible states\n",
    "for x in range(W):\n",
    "    for y in range(L):\n",
    "        for dirc in range(12):\n",
    "            S.append([x,y,dirc])\n",
    "            \n",
    "A = [] # initialize the action space\n",
    "\n",
    "# define possible individual actions\n",
    "SIT = 0\n",
    "FWD = 1\n",
    "BWD = -1\n",
    "NO_TURN = 0\n",
    "L_TURN = -1\n",
    "R_TURN = 1\n",
    "\n",
    "# list all possible combined actions\n",
    "for step in [SIT, FWD, BWD]:\n",
    "    if step != SIT:\n",
    "        for turn in [NO_TURN,L_TURN, R_TURN]:\n",
    "            A.append([step, turn])\n",
    "    else:\n",
    "        A.append([SIT, NO_TURN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns possible next states and their corresponding probabilities based on the current state,  \n",
    "# next action and the (constant) pre-rotate error probability. \n",
    "\n",
    "def next_state_distr(s, a, p_e):\n",
    "    x = s[0]\n",
    "    y = s[1]\n",
    "    dirc = s[2]\n",
    "    step = a[0]\n",
    "    turn = a[1]\n",
    "    S_next_distr = {}\n",
    "    s_index = 0\n",
    "    \n",
    "    if step != 0: \n",
    "        for turn_error in [NO_TURN,L_TURN, R_TURN]: # handle pre-rotate error\n",
    "            x_new = x\n",
    "            y_new = y\n",
    "            s_p_new = []\n",
    "            p_sa = 0\n",
    "            if turn_error == 0:\n",
    "                dirc_new = dirc\n",
    "                p_sa = 1-2*p_e\n",
    "            else:\n",
    "                dirc_new = dirc + turn_error\n",
    "                p_sa = p_e\n",
    "\n",
    "            dirc_new = dirc_new % 12 # translational movement according to the direction\n",
    "            if dirc_new in [11, 0, 1]:\n",
    "                y_new = y + step\n",
    "            elif dirc_new in [2, 3, 4]:\n",
    "                x_new = x + step\n",
    "            elif dirc_new in [5, 6, 7]:\n",
    "                y_new = y - step\n",
    "            else: \n",
    "                x_new = x - step\n",
    "            \n",
    "            dirc_new = (dirc_new + turn) % 12\n",
    "            if x_new < 0 or x_new >= W or y_new < 0 or y_new >= L: # handle attempts to move off grid\n",
    "                s_p_new = [x, y, dirc_new, p_sa]\n",
    "            else:\n",
    "                s_p_new = [x_new, y_new, dirc_new, p_sa] \n",
    "            \n",
    "            S_next_distr[s_index] = s_p_new\n",
    "            s_index += 1\n",
    "\n",
    "    else:\n",
    "        S_next_distr[s_index] = [x, y, dirc, 1]\n",
    "    return S_next_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the probability of the next state based on the current state, \n",
    "# next action and the (constant) pre-rotate error probability. \n",
    "\n",
    "def next_state_p(s, a, s_prime, p_e):\n",
    "    p_sa = 0\n",
    "    S_next_distr = next_state_distr(s, a, p_e)\n",
    "    \n",
    "    # iterate through all possible states in case there are repeated cases\n",
    "    for s_index in S_next_distr:\n",
    "        s_new = S_next_distr[s_index][0:3]\n",
    "        p_new = S_next_distr[s_index][3]\n",
    "        if s_new == s_prime:\n",
    "            p_sa += p_new \n",
    "    return p_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a state that is generated based on the current state, the action \n",
    "# and the (constant) pre-rotate probability.\n",
    "\n",
    "import random \n",
    "\n",
    "def next_state(s, a, p_e):\n",
    "    S_next_distr = next_state_distr(s,a,p_e)\n",
    "    seed = random.random()\n",
    "    prob = 0\n",
    "    \n",
    "    # campare random number with the aggregated probablity of potential next states\n",
    "    for s_index in S_next_distr:\n",
    "        prob += S_next_distr[s_index][3]\n",
    "        if seed < prob:\n",
    "            state = S_next_distr[s_index][0:3]\n",
    "            return state\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the reward given the current state.\n",
    "\n",
    "import numpy\n",
    "\n",
    "def reward(s):\n",
    "    x = s[0]\n",
    "    y = s[1]\n",
    "    \n",
    "    # define rewards all over the map \n",
    "    R = numpy.zeros((W, L))\n",
    "    # define lane markers\n",
    "    for i in [4, 5, 6]:\n",
    "        R[3][i] = -10\n",
    "    # define walls\n",
    "    for i in range(W):\n",
    "        for j in range(L):\n",
    "            if i == 0 or i == 7 or j == 0 or j == 7:\n",
    "                R[i][j] = -100\n",
    "    # define goal sqaure\n",
    "    R[5][6] = 1\n",
    "    \n",
    "    return R[x][y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 For a grid of 8 x 8, there are 768 states.\n"
     ]
    }
   ],
   "source": [
    "print('1.1 For a grid of %d x %d, there are %d states.' % (L, W, len(S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 The robot can choose from 7 actions.\n",
      "['SIT', 'NO_TURN']\n",
      "['FWD', 'NO_TURN']\n",
      "['FWD', 'L_TURN']\n",
      "['FWD', 'R_TURN']\n",
      "['BWD', 'NO_TURN']\n",
      "['BWD', 'L_TURN']\n",
      "['BWD', 'R_TURN']\n"
     ]
    }
   ],
   "source": [
    "print('1.2 The robot can choose from %d actions.' % len(A))\n",
    "for n in A:\n",
    "    print(action_to_words(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the word for an action, based on the input of an action array.\n",
    "# It will be used in testings later. \n",
    "def action_to_words(a):\n",
    "    a_word = []\n",
    "    if a[0] == SIT:\n",
    "        a_word.append('SIT')\n",
    "    elif a[0] == FWD:\n",
    "        a_word.append('FWD')\n",
    "    else:\n",
    "        a_word.append('BWD')\n",
    "    \n",
    "    if a[1] == NO_TURN:\n",
    "        a_word.append('NO_TURN')\n",
    "    elif a[1] == L_TURN:\n",
    "        a_word.append('L_TURN')\n",
    "    else:\n",
    "        a_word.append('R_TURN')\n",
    "    \n",
    "    return a_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3 Test cases: \n",
      "Case 1: current state:[1, 1, 0], action:['SIT', 'NO_TURN'], next state:[1, 1, 0], p_e:0\n",
      "p_sa = 1\n",
      "Case 2: current state:[2, 3, 5], action:['FWD', 'L_TURN'], next state:[2, 2, 4], p_e:0.1\n",
      "p_sa = 0.8\n",
      "Case 3: current state:[7, 7, 6], action:['BWD', 'R_TURN'], next state:[7, 7, 8], p_e:0.2\n",
      "p_sa = 0.2\n",
      "Case 4: current state:[5, 6, 2], action:['BWD', 'NO_TURN'], next state:[4, 6, 3], p_e:0.25\n",
      "p_sa = 0.25\n"
     ]
    }
   ],
   "source": [
    "# Four cases are tested for 1(c). \n",
    "\n",
    "print('1.3 Test cases: ')\n",
    "states = [[1,1,0], [2,3,5], [7,7,6], [5,6,2]]\n",
    "next_states = [[1,1,0], [2,2,4], [7,7,8], [4,6,3]]\n",
    "actions = [[SIT, NO_TURN], [FWD, L_TURN], [BWD, R_TURN], [BWD, NO_TURN]]\n",
    "p_errors = [0, 0.1, 0.2, 0.25]\n",
    "\n",
    "for i in range(len(p_errors)):\n",
    "    s = states[i]\n",
    "    a = actions[i]\n",
    "    s_prime = next_states[i]\n",
    "    p_e = p_errors[i]\n",
    "    p_sa = next_state_p(s, a, s_prime, p_e)\n",
    "    \n",
    "    print('Case %s: current state:%s, action:%s, next state:%s, p_e:%s' \\\n",
    "          % (i+1, s, action_to_words(a), s_prime, p_e))\n",
    "    print('p_sa = %s' % p_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4 Counts of next states generated with 5 run and the sample size of 100:\n",
      "Case 1: current state:[1, 1, 0], action:['SIT', 'NO_TURN'], p_e:0\n",
      "Run 1: p_prime and counts: {'[1, 1, 0]': 100}\n",
      "Run 2: p_prime and counts: {'[1, 1, 0]': 100}\n",
      "Run 3: p_prime and counts: {'[1, 1, 0]': 100}\n",
      "Run 4: p_prime and counts: {'[1, 1, 0]': 100}\n",
      "Run 5: p_prime and counts: {'[1, 1, 0]': 100}\n",
      "Case 2: current state:[2, 3, 5], action:['FWD', 'L_TURN'], p_e:0.1\n",
      "Run 1: p_prime and counts: {'[2, 2, 4]': 74, '[2, 2, 5]': 14, '[3, 3, 3]': 12}\n",
      "Run 2: p_prime and counts: {'[2, 2, 4]': 88, '[3, 3, 3]': 5, '[2, 2, 5]': 7}\n",
      "Run 3: p_prime and counts: {'[2, 2, 4]': 80, '[2, 2, 5]': 10, '[3, 3, 3]': 10}\n",
      "Run 4: p_prime and counts: {'[2, 2, 5]': 13, '[2, 2, 4]': 75, '[3, 3, 3]': 12}\n",
      "Run 5: p_prime and counts: {'[2, 2, 4]': 83, '[2, 2, 5]': 7, '[3, 3, 3]': 10}\n",
      "Case 3: current state:[7, 7, 6], action:['BWD', 'R_TURN'], p_e:0.2\n",
      "Run 1: p_prime and counts: {'[7, 7, 8]': 16, '[7, 7, 7]': 70, '[7, 7, 6]': 14}\n",
      "Run 2: p_prime and counts: {'[7, 7, 6]': 24, '[7, 7, 8]': 24, '[7, 7, 7]': 52}\n",
      "Run 3: p_prime and counts: {'[7, 7, 7]': 55, '[7, 7, 6]': 29, '[7, 7, 8]': 16}\n",
      "Run 4: p_prime and counts: {'[7, 7, 7]': 67, '[7, 7, 6]': 14, '[7, 7, 8]': 19}\n",
      "Run 5: p_prime and counts: {'[7, 7, 6]': 22, '[7, 7, 7]': 62, '[7, 7, 8]': 16}\n",
      "Case 4: current state:[5, 6, 2], action:['BWD', 'NO_TURN'], p_e:0.25\n",
      "Run 1: p_prime and counts: {'[5, 5, 1]': 19, '[4, 6, 2]': 50, '[4, 6, 3]': 31}\n",
      "Run 2: p_prime and counts: {'[5, 5, 1]': 25, '[4, 6, 3]': 30, '[4, 6, 2]': 45}\n",
      "Run 3: p_prime and counts: {'[4, 6, 2]': 56, '[5, 5, 1]': 16, '[4, 6, 3]': 28}\n",
      "Run 4: p_prime and counts: {'[4, 6, 2]': 48, '[4, 6, 3]': 24, '[5, 5, 1]': 28}\n",
      "Run 5: p_prime and counts: {'[4, 6, 3]': 35, '[5, 5, 1]': 28, '[4, 6, 2]': 37}\n"
     ]
    }
   ],
   "source": [
    "# Four cases are tested for 1(d). Distribution of next states generated are displayed. \n",
    "# Test cases are defined in 1(c).\n",
    "\n",
    "S_SIZE = 100\n",
    "RUN = 5\n",
    "print('1.4 Counts of next states generated with %d run and the sample size of %d:' % (RUN, S_SIZE))\n",
    "\n",
    "\n",
    "for i in range(len(p_errors)):\n",
    "    s = states[i]\n",
    "    a = actions[i]\n",
    "    p_e = p_errors[i]\n",
    "    print('Case %s: current state:%s, action:%s, p_e:%s' % (i+1, s, action_to_words(a), p_e))\n",
    "    for r in range(RUN):\n",
    "        next_state_counter = {}\n",
    "        for n in range(S_SIZE):\n",
    "            s_prime = str(next_state(s, a, p_e))\n",
    "\n",
    "            if s_prime in next_state_counter:\n",
    "                next_state_counter[s_prime] += 1\n",
    "            else:\n",
    "                next_state_counter[s_prime] = 1\n",
    "        print('Run %s: p_prime and counts: %s' % (r+1, next_state_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Reward Map\n",
      "-100  -100  -100  -100  -100  -100  -100  -100\n",
      "-100  0     0     -10   0     1     0     -100\n",
      "-100  0     0     -10   0     0     0     -100\n",
      "-100  0     0     -10   0     0     0     -100\n",
      "-100  0     0     0     0     0     0     -100\n",
      "-100  0     0     0     0     0     0     -100\n",
      "-100  0     0     0     0     0     0     -100\n",
      "-100  -100  -100  -100  -100  -100  -100  -100\n"
     ]
    }
   ],
   "source": [
    "# Map for rewards is displayed, assuming the origin is at the bottom left \n",
    "# and x axis to the right. \n",
    "\n",
    "print('2. Reward Map')\n",
    "for j in range(L):    \n",
    "    for i in range(W):\n",
    "        r = int(reward([i,7-j,0]))\n",
    "        if i == 7 :\n",
    "            print(r)\n",
    "        else:\n",
    "            print(r, end = ' ' * (6 - len(str(r))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4(a)\n",
    "def value_iteration(state, pe, discount, theta):\n",
    "    #initialization of Policy and Value.\n",
    "    V = {}\n",
    "    policy = {}\n",
    "    for s in state:\n",
    "        V[s] = 0.0\n",
    "        policy[s] = (0, 0)\n",
    "\n",
    "    while True:\n",
    "        delta = 0       \n",
    "        for s in state:\n",
    "            action_value = 0.0\n",
    "            action = [0, 0]\n",
    "            for a in A:\n",
    "                value = 0.0\n",
    "                state_list = next_state_dist(s, a, pe)\n",
    "                for st in range(len(state_list)):\n",
    "                    test_state = state_list[st][0:3]\n",
    "                    value += next_state_prob(s, a, test_state, pe)*(reward(test_state) + discount * V[test_state])\n",
    "                if value == max(action_value, value):\n",
    "                    action_value = value\n",
    "                    action = a\n",
    "            policy[s] = action\n",
    "            delta = max(delta, abs(action_value - V[s]))\n",
    "            V[s] = action_value         \n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to generate the visual grid world\n",
    "import matplotlib.pyplot as plt\n",
    "def generate_trajectory(policy, s, pe=0.0, show=True):\n",
    "    \"\"\"\n",
    "    Generate a trajectory using policy from initial state s to the goal and\n",
    "    visualize the trajectory on the grid world.\n",
    "    \n",
    "    Args:\n",
    "        Policy: a directionary including all states and their related action.\n",
    "        Initial state s = (x, y, h).\n",
    "        p_e: the error probability Pe to pre-rotate when chosing to move. 0 < Pe <= 0.5\n",
    "    \n",
    "    Returns:\n",
    "        Trajectory including passing states and actions on each state\n",
    "    \"\"\"\n",
    "    \n",
    "    # Confirm the feasibility of p_e\n",
    "    if not (pe >= 0.0 and pe <= 0.5):\n",
    "        raise ValueError('The error probability must be between 0 and 0.5')\n",
    "        \n",
    "    # Generate the trajectory\n",
    "\n",
    "    traj = []\n",
    "    s_now = s\n",
    "    while True:\n",
    "        traj.append([s_now, policy[s_now]])\n",
    "        if (policy[s_now] == [0, 0]):\n",
    "            break\n",
    "        if (s_now[0] == 3) and (s_now[1] == 4):\n",
    "            break\n",
    "        P_state = next_state_dist(s_now, policy[s_now], pe)\n",
    "        statetest = 0\n",
    "        stateindex = 0\n",
    "        for i in range(len(P_state)):\n",
    "            if P_state[i][3] > statetest:\n",
    "                statetest = P_state[i][3]\n",
    "                stateindex = i\n",
    "        s_next = P_state[stateindex]\n",
    "        s_now = s_next[0:3]\n",
    "\n",
    "    \n",
    "    # Grid world initilization\n",
    "    fig = plt.figure(figsize = (L,W))\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    plt.xlim((0, L))\n",
    "    plt.ylim((0, W))\n",
    "    plt.grid(True, color = 'k')\n",
    "\n",
    "    # Plot red markers at edges\n",
    "    edge1 = plt.Rectangle((0,0), 1, 8, color = 'r')\n",
    "    edge2 = plt.Rectangle((1,0), 7, 1, color = 'r')\n",
    "    edge3 = plt.Rectangle((7,1), 1, 7, color = 'r')\n",
    "    edge4 = plt.Rectangle((1,7), 6, 1, color = 'r')\n",
    "    ax.add_patch(edge1)\n",
    "    ax.add_patch(edge2)\n",
    "    ax.add_patch(edge3)\n",
    "    ax.add_patch(edge4)\n",
    "\n",
    "    # Plot yellow markers\n",
    "    yellow1 = plt.Rectangle((3,4), 1, 3, color = 'gold', alpha = 0.8)\n",
    "    ax.add_patch(yellow1)\n",
    "\n",
    "    # Plot green goal\n",
    "    goal = plt.Rectangle((5,6), 1, 1, color = 'limegreen', alpha = 0.8)\n",
    "    ax.add_patch(goal)\n",
    "\n",
    "    # Plot the start state\n",
    "    plt.plot(s[0]+0.5, s[1]+0.5, 'c*', markersize = '40')\n",
    "    ax.arrow(s[0]+0.5, s[1]+0.5, 0.4*np.sin(30*s[2]*np.pi/180),0.4*np.cos(30*s[2]*np.pi/180), \\\n",
    "             head_width = 0.1, head_length = 0.2, fc = 'k', ec = 'k')\n",
    "    \n",
    "    # Plot all passing states\n",
    "    for i in range(1, len(traj)):\n",
    "        x1 = traj[i-1][0][0]\n",
    "        y1 = traj[i-1][0][1]\n",
    "        x2 = traj[i][0][0]\n",
    "        y2 = traj[i][0][1]\n",
    "        h = traj[i][0][2]\n",
    "        plt.plot([x1+0.5, x2+0.5], [y1+0.5, y2+0.5], 'k--')\n",
    "        plt.plot(x2+0.5, y2+0.5, 'bo', markersize = '10')\n",
    "        ax.arrow(x2+0.5, y2+0.5, 0.4*np.sin(30*h*np.pi/180),0.4*np.cos(30*h*np.pi/180), \\\n",
    "                 head_width = 0.1, head_length = 0.2, fc = 'k', ec = 'k')\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'S' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6dd86feb7d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpolicy_optimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0ms0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'S' is not defined"
     ]
    }
   ],
   "source": [
    "# Problem 4(b)\n",
    "import time\n",
    "start_time = time.time()\n",
    "policy_optimal, V_optimal = value_iteration(S, 0.0, 0.9, 0.01)\n",
    "end_time = time.time()\n",
    "s0 = (1, 6, 6)\n",
    "traj_sample = generate_trajectory(policy_optimal, s0, pe=0.0, show=True)\n",
    "\n",
    "V_value_ite = 0\n",
    "for i in range(0, len(traj_sample)):\n",
    "    V_value_ite += V_optimal[traj_sample[i][0]]\n",
    "    print(V_optimal[traj_sample[i][0]])\n",
    "\n",
    "print(\"4(b). Trajectory ([state, action]) from %s to the goal is: \" % str(s0), traj_sample, \", with optimal value:\" ,V_value_ite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
